{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_kW7TOivv29t",
        "-WsQNhIJx-dn"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "r67fHm3M7ju_"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ukBxaw_J-K8"
      },
      "source": [
        "**The following command lines will do:**\n",
        "- Installing the gdown to download things from Google Drive\n",
        "- Download BID dataset (~7GB) from its Google Drive\n",
        "- Unzip the downloaded dataset file\n",
        "- Rename the dataset main folder\n",
        "- Create two folders to start organize the dataset\n",
        "- Create a folder for checkpoints files\n",
        "- Delete the zip file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELEyoXvj8t2G"
      },
      "source": [
        "!pip install gdown\n",
        "!gdown https://drive.google.com/uc?id=1adk1HM2j-HB0YT7UFdt2_xyr0BEdoshx\n",
        "!\n",
        "!unzip /content/BID\\ Dataset.zip\n",
        "!mv /content/BID\\ Dataset/ /content/BID-Dataset/\n",
        "!mkdir /content/BID-Dataset/imgs/ && mkdir /content/BID-Dataset/masks/\n",
        "!mkdir /content/checkpoints/\n",
        "!rm /content/BID\\ Dataset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNVwj8FWKvGk"
      },
      "source": [
        "**Bellow there are two functions:**\n",
        "\n",
        "1.   **get_img_list** -> this function receives as argument the directory of subset which contains images and their masks and returns a list containing the all mentioned files (directory+filename).\n",
        "2.   **organize_dataset** -> this function receives as argument the list of all files and organize all images into the folder \"imgs\" and all masks into the folder \"masks\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liv7Colt8t77"
      },
      "source": [
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "from os import rename\n",
        "from shutil import move\n",
        "\n",
        "def get_img_list(directory):\n",
        "\n",
        "    return glob(directory+'**/*.jpg', recursive=True)\n",
        "\n",
        "\n",
        "def organize_dataset(img_list):\n",
        "\n",
        "    for img_path in tqdm(img_list):\n",
        "\n",
        "        split_path = img_path.split(\"/\")\n",
        "        split_filename = split_path[-1].split(\"_\")\n",
        "\n",
        "        #deleting the namefile from the path\n",
        "        del(split_path[-1])\n",
        "        #deleting the last folder from the path\n",
        "        del(split_path[-1])\n",
        "\n",
        "        #joining the path\n",
        "        new_path = '/'.join(split_path)\n",
        "\n",
        "        #getting the filename with just the id\n",
        "        new_filename = split_filename[0] + \".jpg\"\n",
        "\n",
        "        if len(split_filename) == 3:\n",
        "            #it's a mask!\n",
        "            #example: \"00000000_gt_segmentation.jpg\" -> [\"00000000\", \"gt\", \"segmentation.jpg\"]\n",
        "            new_folder_file = \"/masks/\" + new_filename\n",
        "        else:\n",
        "            #it's a image!\n",
        "            #example: \"00000000_in.jpg\" -> [\"00000000\", \"in.jpg\"]\n",
        "            new_folder_file = \"/imgs/\" + new_filename\n",
        "\n",
        "\n",
        "        #joining the new folder into the new path string\n",
        "        new_path += new_folder_file\n",
        "\n",
        "        move(img_path, new_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERL6Z9MiHMBF"
      },
      "source": [
        "base_dir = \"/content/BID-Dataset/\"\n",
        "subsets = [\"CNH_Aberta/\", \"CNH_Frente/\", \"CNH_Verso/\", \"CPF_Frente/\",\n",
        "           \"CPF_Verso/\", \"RG_Aberto/\", \"RG_Frente/\", \"RG_Verso/\"]\n",
        "\n",
        "for subset in subsets:\n",
        "    all_img = get_img_list(base_dir+subset)\n",
        "    organize_dataset(all_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXWcoOFYlc9J"
      },
      "source": [
        "Here we define a function for encoding the masks, it receives as arguments the list path of masks and the threshold for binarization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RUjisI9lcBE"
      },
      "source": [
        "# import cv2\n",
        "\n",
        "# def hot_encoding_mask(img_list, threshold=100):\n",
        "\n",
        "#     for img_path in tqdm(img_list):\n",
        "\n",
        "#         img = cv2.imread(img_path)\n",
        "#         img = (a > threshold).astype(np.int8)\n",
        "#         cv2.imwrite(img_path, img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYpJyxhulfWD"
      },
      "source": [
        "#masks_folder = \"masks/\"\n",
        "#masks_list = get_img_list(base_dir+masks_folder)\n",
        "#hot_encoding_mask(masks_list, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUpmR4dzMbtR"
      },
      "source": [
        "Bellow we have all the code to build the U-Net.\n",
        "\n",
        "Thanks to the repository: https://github.com/milesial/Pytorch-UNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pmk8M0f8m2MH"
      },
      "source": [
        "\"\"\" Parts of the U-Net model \"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            #nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            #nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mode='baseline'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "\n",
        "        if self.mode == 'scconv':\n",
        "          self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleSCConv(in_channels, out_channels)\n",
        "          )\n",
        "        elif self.mode == 'bottleneck':\n",
        "          self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleBottleneck(in_channels, out_channels)\n",
        "          )\n",
        "        else:\n",
        "          self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "          )\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True, mode='baseline'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "\n",
        "            if self.mode == 'scconv':\n",
        "                self.conv = DoubleSCConv(in_channels, out_channels, None)\n",
        "            elif self.mode == 'bottleneck':\n",
        "                self.conv = DoubleBottleneck(in_channels, out_channels, None)\n",
        "            else:\n",
        "                self.conv = DoubleConv(in_channels, out_channels, None)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "\n",
        "        # if you have padding issues, see\n",
        "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
        "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\"\"\" Full assembly of the parts to form the complete network \"\"\"\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True, mode='baseline'):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "        self.mode = mode\n",
        "\n",
        "        if self.mode == 'scconv':\n",
        "          self.inc = DoubleSCConv(n_channels, 16)\n",
        "        elif self.mode == 'bottleneck':\n",
        "          self.inc = DoubleBottleneck(n_channels, 16) #-- (3,x,y) ---> (16,x,y)\n",
        "        else:\n",
        "          self.inc = DoubleConv(n_channels, 16)\n",
        "\n",
        "        self.down1 = Down(16, 32, self.mode)\n",
        "        self.down2 = Down(32, 64, self.mode)\n",
        "        self.down3 = Down(64, 128, self.mode)\n",
        "        factor = 2 if bilinear else 1\n",
        "        #factor = 1\n",
        "\n",
        "        self.down4 = Down(128, 256 // factor, self.mode)\n",
        "        self.up1 = Up(256, 128 // factor, bilinear, self.mode)\n",
        "        self.up2 = Up(128, 64 // factor, bilinear, self.mode)\n",
        "        self.up3 = Up(64, 32 // factor, bilinear, self.mode)\n",
        "        self.up4 = Up(32, 16, bilinear, self.mode)\n",
        "        self.outc = OutConv(16, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5 = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBGViPOSZNXt"
      },
      "source": [
        "class SCBottleneck(nn.Module):\n",
        "    \"\"\"SCNet SCBottleneck\n",
        "    \"\"\"\n",
        "    expansion = 4\n",
        "    pooling_r = 4 # down-sampling rate of the avg pooling layer in the K3 path of SC-Conv.\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None,\n",
        "                 cardinality=1, bottleneck_width=32,\n",
        "                 avd=False, dilation=1, is_first=False,\n",
        "                 norm_layer=nn.BatchNorm2d):\n",
        "        super(SCBottleneck, self).__init__()\n",
        "        group_width = int(planes * (bottleneck_width / 64.)) * cardinality\n",
        "        self.conv1_a = nn.Conv2d(inplanes, group_width, kernel_size=1, bias=False)\n",
        "        self.bn1_a = norm_layer(group_width)\n",
        "        self.conv1_b = nn.Conv2d(inplanes, group_width, kernel_size=1, bias=False)\n",
        "        self.bn1_b = norm_layer(group_width)\n",
        "        self.avd = avd and (stride > 1 or is_first)\n",
        "\n",
        "        if self.avd:\n",
        "            self.avd_layer = nn.AvgPool2d(3, stride, padding=1)\n",
        "            stride = 1\n",
        "\n",
        "        self.k1 = nn.Sequential(\n",
        "                    nn.Conv2d(\n",
        "                        group_width, group_width, kernel_size=3, stride=stride,\n",
        "                        padding=dilation, dilation=dilation,\n",
        "                        groups=cardinality, bias=False),\n",
        "                    norm_layer(group_width),\n",
        "                    )\n",
        "\n",
        "        self.scconv = SCConv(group_width, group_width, kernel_size=3, padding=1)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(\n",
        "            group_width * 2, planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = norm_layer(planes)\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.dilation = dilation\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out_a= self.conv1_a(x)\n",
        "        out_a = self.bn1_a(out_a)\n",
        "\n",
        "        out_b = self.conv1_b(x)\n",
        "        out_b = self.bn1_b(out_b)\n",
        "\n",
        "        out_a = self.relu(out_a)\n",
        "        out_b = self.relu(out_b)\n",
        "\n",
        "        out_a = self.k1(out_a)\n",
        "        out_b = self.scconv(out_b)\n",
        "\n",
        "        out_a = self.relu(out_a)\n",
        "        out_b = self.relu(out_b)\n",
        "\n",
        "        if self.avd:\n",
        "            out_a = self.avd_layer(out_a)\n",
        "            out_b = self.avd_layer(out_b)\n",
        "\n",
        "        out = self.conv3(torch.cat([out_a, out_b], dim=1))\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class DoubleBottleneck(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            #SCConv(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            #SCConv(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            SCBottleneck(mid_channels, out_channels, stride=1, downsample=None,\n",
        "                 cardinality=1, bottleneck_width=32,\n",
        "                 avd=False, dilation=1, is_first=False,\n",
        "                 norm_layer=nn.BatchNorm2d),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class DoubleSCConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            SCConv(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            #nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "\n",
        "            SCConv(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "class SCConv(nn.Module):\n",
        "\n",
        "    def __init__(self, in_channels, mid_channels=None, kernel_size=3, padding=1):\n",
        "        super(SCConv, self).__init__()\n",
        "        #print(in_channels,mid_channels)\n",
        "\n",
        "        self.k2 = nn.Sequential(\n",
        "                    nn.AvgPool2d(kernel_size=4, stride=4),\n",
        "                    nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, dilation=1, stride=1, bias=False, groups=1),\n",
        "                    nn.BatchNorm2d(in_channels),\n",
        "                    )\n",
        "        self.k3 = nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, dilation=1, stride=1, bias=False, groups=1),\n",
        "                    nn.BatchNorm2d(in_channels),\n",
        "                    )\n",
        "        self.k4 = nn.Sequential(\n",
        "                    nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, dilation=1, stride=1, bias=False, groups=1),\n",
        "                    nn.BatchNorm2d(mid_channels),\n",
        "                    )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "\n",
        "        out = torch.sigmoid(torch.add(identity, F.interpolate(self.k2(x), identity.size()[2:]))) # sigmoid(identity + k2)\n",
        "        out = torch.mul(self.k3(x), out) # k3 * sigmoid(identity + k2)\n",
        "        out = self.k4(out) # k4\n",
        "\n",
        "        return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JttCA3Rn9H_Q"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cIwyy_du8fp"
      },
      "source": [
        "from torchsummary import summary\n",
        "net = UNet(n_channels=1, n_classes=1, bilinear=True, mode='scconv')\n",
        "net.to(device=device)\n",
        "#print(net)\n",
        "summary(net, (1, 512, 512))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqP8HHtrNMg-"
      },
      "source": [
        "Bellow we have all the code to build the dataset to be used in our experiments.\n",
        "\n",
        "Thanks to the repository: https://github.com/milesial/Pytorch-UNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4Hua7xDqqtP"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "from os import listdir\n",
        "from os.path import splitext\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "class BasicDataset(Dataset):\n",
        "    def __init__(self, imgs_dir, masks_dir, scale=1, mask_suffix=''):\n",
        "        self.imgs_dir = imgs_dir\n",
        "        self.masks_dir = masks_dir\n",
        "        self.scale = scale\n",
        "        self.mask_suffix = mask_suffix\n",
        "        assert 0 < scale <= 1, 'Scale must be between 0 and 1'\n",
        "\n",
        "        self.ids = [splitext(file)[0] for file in listdir(imgs_dir)\n",
        "                    if not file.startswith('.')]\n",
        "        logging.info(f'Creating dataset with {len(self.ids)} examples')\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    @classmethod\n",
        "    def preprocess(cls, pil_img, scale, mask_flag=False):\n",
        "\n",
        "        w, h = pil_img.size\n",
        "        newW, newH = int(scale * w), int(scale * h)\n",
        "        assert newW > 0 and newH > 0, 'Scale is too small'\n",
        "        pil_img = pil_img.resize((newW, newH))\n",
        "\n",
        "        img_nd = np.array(pil_img)\n",
        "        img_nd = cv2.resize(img_nd, (512,512), interpolation = cv2.INTER_AREA)\n",
        "        #img_nd = cv2.resize(img_nd, (506,506), interpolation = cv2.INTER_AREA)\n",
        "\n",
        "        if len(img_nd.shape) == 2:\n",
        "            img_nd = np.expand_dims(img_nd, axis=2)\n",
        "\n",
        "        if mask_flag:\n",
        "            msk = (img_nd > 100).astype(np.uint8)\n",
        "            return msk.transpose((2, 0, 1))\n",
        "\n",
        "        # HWC to CHW\n",
        "        img_trans = img_nd.transpose((2, 0, 1))\n",
        "        if img_trans.max() > 1:\n",
        "            img_trans = img_trans / 255\n",
        "\n",
        "        return img_trans\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        idx = self.ids[i]\n",
        "        mask_file = glob(self.masks_dir + idx + self.mask_suffix + '.*')\n",
        "        img_file = glob(self.imgs_dir + idx + '.*')\n",
        "\n",
        "        assert len(mask_file) == 1, \\\n",
        "            f'Either no mask or multiple masks found for the ID {idx}: {mask_file}'\n",
        "        assert len(img_file) == 1, \\\n",
        "            f'Either no image or multiple images found for the ID {idx}: {img_file}'\n",
        "\n",
        "        #these two lines bellow are for image opening\n",
        "        mask = Image.open(mask_file[0])\n",
        "        img = Image.open(img_file[0])\n",
        "\n",
        "        #grayscale image adaptation here\n",
        "        red, green, blue = img.split()\n",
        "        img = green\n",
        "\n",
        "        red_msk, green_msk, blue_msk = mask.split()\n",
        "        mask = green_msk\n",
        "\n",
        "\n",
        "        assert img.size == mask.size, \\\n",
        "            f'Image and mask {idx} should be the same size, but are {img.size} and {mask.size}'\n",
        "\n",
        "        img = self.preprocess(img, self.scale, mask_flag = False)\n",
        "        mask = self.preprocess(mask, self.scale, mask_flag = True)\n",
        "\n",
        "        #print(np.unique(mask))\n",
        "\n",
        "        return {\n",
        "            'image': torch.from_numpy(img).type(torch.FloatTensor),\n",
        "            'mask': torch.from_numpy(mask).type(torch.FloatTensor)\n",
        "        }\n",
        "\n",
        "\n",
        "class CarvanaDataset(BasicDataset):\n",
        "    def __init__(self, imgs_dir, masks_dir, scale=1):\n",
        "        super().__init__(imgs_dir, masks_dir, scale, mask_suffix='_mask')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6q6jezoV2b5G"
      },
      "source": [
        "Bellow there is a code for evaluate our model.\n",
        "Thanks to the repository (modified): https://github.com/milesial/Pytorch-UNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NZVU5Z8y8dL"
      },
      "source": [
        "from torch.autograd import Function\n",
        "from sklearn.metrics import jaccard_similarity_score as iou\n",
        "\n",
        "\n",
        "def transform(tensor):\n",
        "    return tensor.cpu().numpy().flatten()\n",
        "\n",
        "\n",
        "def evaluation_v2(net, loader, device):\n",
        "    \"\"\"Evaluation without the densecrf with the dice coefficient\"\"\"\n",
        "    net.eval()\n",
        "    mask_type = torch.float32 if net.n_classes == 1 else torch.long\n",
        "    n_val = len(loader)  # the number of batch\n",
        "    tot_iou = 0\n",
        "    tot_dice = 0\n",
        "\n",
        "    #for batch in tqdm(loader):\n",
        "    print(\"evaluating...\")\n",
        "    for batch in tqdm(loader):\n",
        "        imgs, true_masks = batch['image'], batch['mask']\n",
        "        imgs = imgs.to(device=device, dtype=torch.float32)\n",
        "        true_masks = true_masks.to(device=device, dtype=mask_type)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            mask_pred = net(imgs)\n",
        "\n",
        "        if net.n_classes > 1:\n",
        "            tot += F.cross_entropy(mask_pred, true_masks).item()\n",
        "        else:\n",
        "            pred = torch.sigmoid(mask_pred)\n",
        "            pred = (pred > 0.5).float()\n",
        "\n",
        "            tot_dice += dice_coeff(pred, true_masks).item()\n",
        "            tot_iou += iou(transform(pred), transform(true_masks))\n",
        "\n",
        "    net.train()\n",
        "    return (tot_dice / n_val) , (tot_iou / n_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__dcpOhENXbc"
      },
      "source": [
        "Bellow we have all the code to make the training process with the proposed U-Net in the dataset organized.\n",
        "\n",
        "Thanks to the repository: https://github.com/milesial/Pytorch-UNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l0KtyF8IqR3y"
      },
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch import optim\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Function\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "def train_net(net,\n",
        "              device,\n",
        "              epochs=5,\n",
        "              batch_size=1,\n",
        "              lr=0.001,\n",
        "              val_percent=0.1,\n",
        "              save_cp=True,\n",
        "              img_scale=0.5,\n",
        "              dir_checkpoint=\"\",\n",
        "              mode = ''):\n",
        "\n",
        "    dataset = BasicDataset(dir_img, dir_mask, img_scale)\n",
        "    n_val = int(len(dataset) * val_percent)\n",
        "    n_train = len(dataset) - n_val\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    train, val = random_split(dataset, [n_train, n_val])\n",
        "    train_loader = DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
        "    val_loader = DataLoader(val, batch_size=batch_size, shuffle=False, num_workers=8, pin_memory=True, drop_last=True)\n",
        "\n",
        "    writer = SummaryWriter(comment=f'LR_{lr}_BS_{batch_size}_SCALE_{img_scale}')\n",
        "    global_step = 0\n",
        "\n",
        "    logging.info(f'''Starting training:\n",
        "        Epochs:          {epochs}\n",
        "        Batch size:      {batch_size}\n",
        "        Learning rate:   {lr}\n",
        "        Training size:   {n_train}\n",
        "        Validation size: {n_val}\n",
        "        Checkpoints:     {save_cp}\n",
        "        Device:          {device.type}\n",
        "        Images scaling:  {img_scale}\n",
        "    ''')\n",
        "\n",
        "    optimizer = optim.RMSprop(net.parameters(), lr=lr, weight_decay=1e-8, momentum=0.9)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min' if net.n_classes > 1 else 'max', patience=2)\n",
        "    if net.n_classes > 1:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "    else:\n",
        "        criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        net.train()\n",
        "\n",
        "        epoch_loss = 0\n",
        "        with tqdm(total=n_train, desc=f'Epoch {epoch + 1}/{epochs}', unit='img') as pbar:\n",
        "            for batch in train_loader:\n",
        "                imgs = batch['image']\n",
        "                true_masks = batch['mask']\n",
        "                assert imgs.shape[1] == net.n_channels, \\\n",
        "                    f'Network has been defined with {net.n_channels} input channels, ' \\\n",
        "                    f'but loaded images have {imgs.shape[1]} channels. Please check that ' \\\n",
        "                    'the images are loaded correctly.'\n",
        "\n",
        "                imgs = imgs.to(device=device, dtype=torch.float32)\n",
        "                mask_type = torch.float32 if net.n_classes == 1 else torch.long\n",
        "                true_masks = true_masks.to(device=device, dtype=mask_type)\n",
        "\n",
        "                masks_pred = net(imgs)\n",
        "                loss = criterion(masks_pred, true_masks)\n",
        "                epoch_loss += loss.item()\n",
        "                writer.add_scalar('Loss/train', loss.item(), global_step)\n",
        "\n",
        "                pbar.set_postfix(**{'loss (batch)': loss.item()})\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                nn.utils.clip_grad_value_(net.parameters(), 0.1)\n",
        "                optimizer.step()\n",
        "\n",
        "                pbar.update(imgs.shape[0])\n",
        "                global_step += 1\n",
        "                '''\n",
        "                if global_step % (n_train // (50 * batch_size)) == 0:\n",
        "                    for tag, value in net.named_parameters():\n",
        "                        tag = tag.replace('.', '/')\n",
        "                        writer.add_histogram('weights/' + tag, value.data.cpu().numpy(), global_step)\n",
        "                        writer.add_histogram('grads/' + tag, value.grad.data.cpu().numpy(), global_step)\n",
        "                    val_score = eval_net(net, val_loader, device)\n",
        "                    print(\"Validation result is:\")\n",
        "\n",
        "\n",
        "                    scheduler.step(val_score)\n",
        "                    writer.add_scalar('learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
        "\n",
        "                    if net.n_classes > 1:\n",
        "                        logging.info('Validation cross entropy: {}'.format(val_score))\n",
        "                        writer.add_scalar('Loss/test', val_score, global_step)\n",
        "                    else:\n",
        "                        logging.info('Validation Dice Coeff: {}'.format(val_score))\n",
        "                        writer.add_scalar('Dice/test', val_score, global_step)\n",
        "\n",
        "                    writer.add_images('images', imgs, global_step)\n",
        "                    if net.n_classes == 1:\n",
        "                        writer.add_images('masks/true', true_masks, global_step)\n",
        "                        writer.add_images('masks/pred', torch.sigmoid(masks_pred) > 0.5, global_step)\n",
        "                '''\n",
        "\n",
        "        #descomentar esses caras abaixo quando tiver tudo ok\n",
        "        #val_score = eval_net(net, val_loader, device)\n",
        "        #print(\"Validation result is:\")\n",
        "        #print(val_score)\n",
        "        dice, iou = evaluation_v2(net, val_loader, device)\n",
        "        print(\"Validation result is:\")\n",
        "        print(dice, iou)\n",
        "\n",
        "        if save_cp:\n",
        "            try:\n",
        "                os.mkdir(dir_checkpoint)\n",
        "                logging.info('Created checkpoint directory')\n",
        "            except OSError:\n",
        "                pass\n",
        "\n",
        "            fname_cp = mode + '_' + f'CP_epoch{epoch + 1}.pth'\n",
        "\n",
        "            cp_file = dir_checkpoint + fname_cp\n",
        "            torch.save(net.state_dict(), cp_file)\n",
        "            print(\"checkpoint saved!\")\n",
        "\n",
        "            destination = '/content/drive/MyDrive/Projeto-DL/' + fname_cp\n",
        "            shutil.move(cp_file, destination)\n",
        "            print(\"checkpoint downloaded!\")\n",
        "\n",
        "\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "    return net, train_loader, val_loader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PKOZgshC19U1"
      },
      "source": [
        "from torch.autograd import Function\n",
        "\n",
        "from sklearn.metrics import jaccard_similarity_score as iou\n",
        "\n",
        "class DiceCoeff(Function):\n",
        "    \"\"\"Dice coeff for individual examples\"\"\"\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        self.save_for_backward(input, target)\n",
        "        eps = 0.0001\n",
        "        self.inter = torch.dot(input.view(-1), target.view(-1))\n",
        "        self.union = torch.sum(input) + torch.sum(target) + eps\n",
        "\n",
        "        t = (2 * self.inter.float() + eps) / self.union.float()\n",
        "        return t\n",
        "\n",
        "    # This function has only a single output, so it gets only one gradient\n",
        "    def backward(self, grad_output):\n",
        "\n",
        "        input, target = self.saved_variables\n",
        "        grad_input = grad_target = None\n",
        "\n",
        "        if self.needs_input_grad[0]:\n",
        "            grad_input = grad_output * 2 * (target * self.union - self.inter) \\\n",
        "                         / (self.union * self.union)\n",
        "        if self.needs_input_grad[1]:\n",
        "            grad_target = None\n",
        "\n",
        "        return grad_input, grad_target\n",
        "\n",
        "\n",
        "def dice_coeff(input, target):\n",
        "    \"\"\"Dice coeff for batches\"\"\"\n",
        "    if input.is_cuda:\n",
        "        s = torch.FloatTensor(1).cuda().zero_()\n",
        "    else:\n",
        "        s = torch.FloatTensor(1).zero_()\n",
        "\n",
        "    for i, c in enumerate(zip(input, target)):\n",
        "        s = s + DiceCoeff().forward(c[0], c[1])\n",
        "\n",
        "    return s / (i + 1)\n",
        "\n",
        "def iou_pytorch(outputs: torch.Tensor, labels: torch.Tensor):\n",
        "    # You can comment out this line if you are passing tensors of equal shape\n",
        "    # But if you are passing output from UNet or something it will most probably\n",
        "    # be with the BATCH x 1 x H x W shape\n",
        "    outputs = outputs.squeeze(1)  # BATCH x 1 x H x W => BATCH x H x W\n",
        "\n",
        "    intersection = (outputs & labels).float().sum((1, 2))  # Will be zero if Truth=0 or Prediction=0\n",
        "    union = (outputs | labels).float().sum((1, 2))         # Will be zzero if both are 0\n",
        "\n",
        "    iou = (intersection + SMOOTH) / (union + SMOOTH)  # We smooth our devision to avoid 0/0\n",
        "\n",
        "    thresholded = torch.clamp(20 * (iou - 0.5), 0, 10).ceil() / 10  # This is equal to comparing with thresolds\n",
        "\n",
        "    return thresholded\n",
        "\n",
        "def eval_net(net, loader, device):\n",
        "    \"\"\"Evaluation without the densecrf with the dice coefficient\"\"\"\n",
        "    net.eval()\n",
        "    mask_type = torch.float32 if net.n_classes == 1 else torch.long\n",
        "    n_val = len(loader)  # the number of batch\n",
        "    tot = 0\n",
        "\n",
        "    #for batch in tqdm(loader):\n",
        "    print(\"evaluating...\")\n",
        "    for batch in loader:\n",
        "        imgs, true_masks = batch['image'], batch['mask']\n",
        "        imgs = imgs.to(device=device, dtype=torch.float32)\n",
        "        true_masks = true_masks.to(device=device, dtype=mask_type)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            mask_pred = net(imgs)\n",
        "\n",
        "        if net.n_classes > 1:\n",
        "            tot += F.cross_entropy(mask_pred, true_masks).item()\n",
        "        else:\n",
        "            pred = torch.sigmoid(mask_pred)\n",
        "            pred = (pred > 0.5).float()\n",
        "            #tot += dice_coeff(pred, true_masks).item()\n",
        "            iou_actual = iou_pytorch(pred, true_masks)\n",
        "            tot += iou_actual\n",
        "            print(iou_actual)\n",
        "\n",
        "    net.train()\n",
        "    return tot / n_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qqmqd2YONiI1"
      },
      "source": [
        "Bellow we have all the code to set all necessary variables to make the training:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54dDjl3FNDvG"
      },
      "source": [
        "import logging\n",
        "\n",
        "epochs=5\n",
        "batch_size=5\n",
        "learning_rate=0.0001\n",
        "load=False\n",
        "scale=1\n",
        "validation=25.0\n",
        "\n",
        "mode='scconv'\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "logging.info(f'Using device {device}')\n",
        "\n",
        "#net = UNet(n_channels=1, n_classes=1, bilinear=True)\n",
        "#net = UNet(n_channels=1, n_classes=1, bilinear=True, useSCConv=True)\n",
        "net = UNet(n_channels=1, n_classes=1, bilinear=True, mode=mode)\n",
        "net.to(device=device)\n",
        "\n",
        "dir_img = '/content/BID-Dataset/imgs/'\n",
        "dir_mask = '/content/BID-Dataset/masks/'\n",
        "dir_checkpoint = '/content/checkpoints/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kUMqsHrfDqQ"
      },
      "source": [
        "net, data_train, data_val = train_net(net=net, epochs=100,\n",
        "                              batch_size=batch_size,\n",
        "                              lr=learning_rate, device=device,\n",
        "                              img_scale=scale,\n",
        "                              val_percent=validation / 100,\n",
        "                              dir_checkpoint = dir_checkpoint,\n",
        "                              mode = mode)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7quwgO68RG-_"
      },
      "source": [
        "Code for prediction of images!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pCi4Ul-RKIF"
      },
      "source": [
        "from PIL import Image\n",
        "from matplotlib.pyplot import imshow\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def imshow_v2(img, binary=False):\n",
        "\n",
        "  if binary:\n",
        "    img = 1- img\n",
        "\n",
        "  plt.subplot()\n",
        "  plt.imshow(img, cmap='Greys',  interpolation='nearest')\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def get_net(cp_model, mode):\n",
        "\n",
        "    net = UNet(n_channels=1, n_classes=1, bilinear=True, mode=mode)\n",
        "    net.to(device=device)\n",
        "\n",
        "    net.load_state_dict(torch.load(cp_model, map_location=device))\n",
        "\n",
        "    return net\n",
        "\n",
        "def open_image(img_file, split = True):\n",
        "\n",
        "    img = Image.open(img_file)\n",
        "    red, green, blue = img.split()\n",
        "\n",
        "    return img, green\n",
        "\n",
        "def predict_img(net,\n",
        "                full_img,\n",
        "                device,\n",
        "                scale_factor=1,\n",
        "                out_threshold=0.5):\n",
        "    net.eval()\n",
        "\n",
        "    img = torch.from_numpy(BasicDataset.preprocess(full_img, scale_factor))\n",
        "\n",
        "    img = img.unsqueeze(0)\n",
        "    img = img.to(device=device, dtype=torch.float32)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = net(img)\n",
        "\n",
        "        if net.n_classes > 1:\n",
        "            probs = F.softmax(output, dim=1)\n",
        "        else:\n",
        "            probs = torch.sigmoid(output)\n",
        "\n",
        "        probs = probs.squeeze(0)\n",
        "\n",
        "        tf = transforms.Compose(\n",
        "            [\n",
        "                transforms.ToPILImage(),\n",
        "                transforms.Resize(full_img.size[1]),\n",
        "                transforms.ToTensor()\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        probs = tf(probs.cpu())\n",
        "        full_mask = probs.squeeze().cpu().numpy()\n",
        "\n",
        "    return (full_mask > out_threshold).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my9xUnsh3fPh"
      },
      "source": [
        "# Nova seção"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEDjOybNTdXM"
      },
      "source": [
        "#tem que fazer o upload (caso não tenha treinado) do arquivo dos pesos\n",
        "cp_model = dir_checkpoint + \"CP_epoch5.pth\"\n",
        "#net = (cp_model, \"scconv\")\n",
        "#print(net)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKK8XCjf5A2p"
      },
      "source": [
        "net = UNet(n_channels=1, n_classes=1, bilinear=True, mode=\"botleneck\")\n",
        "net.to(device=device)\n",
        "net.load_state_dict(torch.load(cp_model, map_location=device))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUP9JGZ5WgfC"
      },
      "source": [
        "f = \"00020001.jpg\"\n",
        "img_file = dir_img + f\n",
        "original_mask = dir_mask + f\n",
        "\n",
        "imgrgb, img = open_image(img_file)\n",
        "\n",
        "\n",
        "mask = predict_img(net=net, full_img=img,\n",
        "                  scale_factor=0.5,\n",
        "                  out_threshold=0.5,\n",
        "                  device=device)\n",
        "\n",
        "img = img.resize((mask.shape[0],mask.shape[1]))\n",
        "imgrgb = imgrgb.resize((mask.shape[0],mask.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tY17DO5cLnaw"
      },
      "source": [
        "imshow_v2(imgrgb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVMvXEJaNhfR"
      },
      "source": [
        "imshow_v2(mask, binary = True)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}